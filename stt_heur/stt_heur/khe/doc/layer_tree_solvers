@Chapter
    @PartNumber { Part B }
    @PartTitle { Some Solvers }
    @PartText {
A solver is an operation that makes large-scale changes to a solution.
Solvers operate at a high level and should not be cluttered with
implementation details:  their source files will include @C { khe.h }
as usual, but should not include header file @C { khe_interns.h } which
gives access to KHE's internals.  Thus, the user of KHE is as well
equipped to write a solver as its author.
@LP
Many solvers are packaged with KHE.  They are the subject of this
part of the manual, all of which is implemented using @C { khe.h }
but not @C { khe_interns.h }.
# @LP
# There are four broad categories of solvers.  A @I { structural }
# solver organizes a solution for solving, or reorganizes it.  For
# example, it might place meets into nodes, attach monitors, and
# so on.  An @I { assignment } solver assigns meets or tasks without
# changing any existing assignments.  A @I { repair } solver also
# assigns meets or tasks, but it is free to change existing
# assignments as well.
}
    @Title { Layer Tree Solvers }
    @Tag { layer_tree_solvers }
@Begin
@LP
This chapter documents the solvers packaged with KHE that build and
restructure layer trees.
@BeginSections

@Section
    @Title { Layer tree construction }
    @Tag { layer_tree_solvers.construction }
@Begin
@LP
KHE offers a solver for building a layer tree
# .  Before
# presenting that function, however, we need to clear up a small issue.
# @PP
# When constructing a layer tree, ideally no meets would be
# assigned to other meets, since assignment is a separate problem,
# the province of a time assignment algorithm.  But consider the case of
# an elective which offers the senior students a choice of, say, six
# different subjects, of equal durations, running simultaneously.  In the
# instance, these six subjects will be represented by six events linked to
# each other (or to a container event) by link events constraints.  Now
# suppose that each subject needs to be split into, say, four solution
# events.  Each set of four meets must not overlap in time, so
# needs to be placed into a layer.  But this is wasteful of layer indexes,
# given that a time assignment algorithm will later assign them to a
# container event's or other elective's meets in a trivial way
# that guarantees that they will not overlap.
# @PP
# Accordingly, in trivial cases the function makes the assignments
# and omits the layers.  On one instance, this simple optimization
# reduced the number of layers from a worryingly large 270 to just 33.
# According to Chapter {@NumberOf time_solvers}, a meet whose
# assignment should never be changed should be removed from its node,
# to signal this fact to other solvers, and that is done here.
# @PP
# To construct a layer tree
holding the meets of a given solution:
@ID @C {
KHE_NODE KheLayerTreeMake(KHE_SOLN soln,
  bool check_prefer_times_monitors, bool check_split_events_monitors,
  bool check_link_events_monitors);
}
The root node of the tree, holding the cycle meets, is
returned.
# This function is a solver in the sense
# of Section {@NumberOf solutions_overview}:  it makes a large-scale
# change to a solution (it adds a layer tree to it).
The function has no special access to data behind the scenes.  Instead,
it works by calling basic operations and helper functions:
@BulletList

@LI {
It calls @C { KheMeetSplit } to satisfy split events constraints and
other influences on the number and duration of meets, as far as
possible.  It is usual to call @C { KheLayerTreeMake } when each
event is represented in @C { soln } by a single meet of the full
duration (that is, after @C { KheSolnMake } and
@C { KheSolnMakeCompleteRepresentation }), but some meets may be
already split.  In any case, @C { KheLayerTreeMake } does not create,
delete, or merge meets.
}

@LI {
It calls @C { KheMeetSetDomain } to set the time domains
of meets to satisfy preassigned times, prefer times
constraints, and other influences on time domains, as far as
possible.  It is usual to call @C { KheLayerTreeMake } at a
moment when the time domains of the meets are
unrestrictive, but some meets may already have more
restrictive domains.  In any case, @C { KheLayerTreeMake } never
adds a time to an existing time domain:  it either leaves a
domain unchanged, or reduces it to a subset of its initial value.
}

@LI {
It calls @C { KheMeetAssign } in trivial cases where there is
no doubt that the assignments will be final.  Precisely, if there
are two events of equal duration linked by a link events constraint
and split into meets of equal durations, and the algorithm
places one in a parent node and the other in a child of that parent,
then, provided the child node itself has no children (which would
render the case non-trivial), the meets of the child node
will be assigned to meets of the parent node, and the child
node will be deleted in accordance with the convention given
in Chapter {@NumberOf time_solvers}, that meets whose assignments
will never change should not lie in nodes.
# On return, every meet will either lie in a node or be
# assigned to another meet.
}

# @LI {
# As discussed above, it calls @C { KheMeetAssign } to avoid
# creating layers in trivial cases where there is no doubt that
# the assignments will be final.  Precisely, if there are two
# events of equal duration linked by a link events constraint and
# split into meets of equal durations, and the algorithm
# places one in a parent node and the other in a child of that
# parent, then, provided the child node itself has no children
# (which would render the case non-trivial), the meets
# of the child node will be assigned to meets of the
# parent node instead of being placed into a shared layer, and
# the child node will be deleted.
# }

# @LI {
# It calls @C { KheLayerMake } and @C { KheLayerAddMeet }
# to create the layers described below. Every meet
# will either lie in at least one layer, or else it will receive
# one of the trivial assignments just described.  It is usual to
# call @C { KheLayerTreeMake } at a moment when there are no
# layers; in any case, any existing layers will be preserved in
# the sense that any meets that share a layer before
# @C { KheLayerTreeMake } is called will share a layer afterwards,
# although not necessarily the same layer.
# }

@LI {
It calls @C { KheNodeMake } and @C { KheNodeAddMeet } to
ensure that for each event there is one node holding exactly the
meets of that event, unless these meets receive
the trivial assignments just described.  There is also a node
(the root node returned by @C { KheLayerTreeMake }, and also
accessible as @C { KheSolnNode(soln, 0) }) holding exactly the
cycle meets.  Any other meets (usually none)
are not placed into nodes.  It is a precondition of
@C { KheLayerTreeMake } that @C { soln } contain no nodes
when it is called.
}

@LI {
It calls @C { KheNodeAddParent } to reflect link events
constraints (even between events whose durations differ), as
far as possible, and the need to ultimately assign every meet
to a cycle meet.  When @C { KheLayerTreeMake } returns, every
node is a descendant of the root node.
}

@LI {
If requested by @C { check_prefer_times_monitors }, it calls
@C { KheMonitorAttachCheck } on all prefer times monitors,
consistent with the assignment and grouping invariant
(Section {@NumberOf grouping.conventions}).
}

@LI {
If requested by @C { check_split_events_monitors }, it calls
@C { KheMonitorAttachCheck } on all split events monitors and
distribute split events monitors, consistent with the assignment
and grouping invariant (Section {@NumberOf grouping.conventions}).
}

@LI {
If requested by @C { check_link_events_monitors }, it calls
@C { KheMonitorAttachCheck } on all link events monitors,
consistent with the assignment and grouping invariant
(Section {@NumberOf grouping.conventions}).
}

# @LI {
# If requested by @C { detach_prefer_resources_monitors }, it calls
# @C { KheMonitorDetach } to detach all attached prefer resources
# monitors whose cost is guaranteed to be 0 by the domains of the
# tasks they monitor.  This is appropriate when the domains assigned
# to tasks by @C { KheLayerTreeMake } will not be changed, except
# possibly to subsets of their values.
# }

@EndList
These elements interact in ways that make most of them impossible
to separate.  For example, the splitting of an event into
meets needs to be influenced not just by the event's own split
events constraints and distribute split events constraints, but
also by the constraints of the events that it is linked to by
link events constraints.
# @PP
# The layers created by @C { KheLayerTreeMake } aim to satisfy four
# conditions:  for each resource that a required avoid clashes constraint
# applies to, the meets of the events preassigned that resource
# have a layer in common; for each event, the meets of that
# event are either all assigned to other meets which are
# guaranteed to be disjoint in time, or else they lie in a common layer;
# every meet (whether derived from an instance event or not)
# not assigned to another meet lies in at least one layer; and
# there are no redundant layers.  Where appropriate, layers created by
# @C { KheLayerTreeMake } contain non-@C { NULL } resource attributes.
# @PP
# Solution events that share a layer cannot overlap in time.  So the first
# condition prevents required avoid clashes constraint violations involving
# preassigned resources, and the second prevents the meets
# of one event from overlapping in time.  An absolute guarantee of
# these conditions cannot be given, however, because there are unusual
# cases where they are not achievable, such as when the total duration
# of the events containing a given preassigned resource, or the total
# duration of the meets of an event, exceeds the number of
# times in the instance.  The third condition, which @I is guaranteed,
# is for convenience:  it ensures that if the unassigned solution
# events are assigned later layer by layer, no meets are
# missed.  The fourth condition, also guaranteed, is for efficiency:
# as explained in Section {@NumberOf layer_trees.layers}, reducing the
# number of layers reduces the cost of operations on sets of layers.
# @PP
# Although student group resources make the best layers, no attempt is
# made to identify them and treat them specially.  The XML format does
# not make it easy, and there seems to be no reason to do it anyway,
# provided redundant layers are removed.  In practice, most of the layers
# produced by @C { KheLayerTreeMake } have a non-@C { NULL } resource
# attribute holding a student group resource.
# @PP
# No assignments of one meet to another are made, even in
# cases where the node rule dictates what these assignments must be.
# These are left to time assignment algorithms.
@PP
Layers (sets of events that share a common preassigned resource
which has a hard avoid clashes constraint) have a strong influence
on the behaviour of @C { KheLayerTreeMake }.  It ensures that
the events of each layer are split into meets which
can be packed into the cycle meets without overlapping
in time, except in the unlikely case where the total duration of
the events of the layer exceeds the total number of times in
the cycle.
@PP
For each @C { meet } with a pre-existing assignment to some
@C { target_meet }, @C { KheLayerTreeMake } tries to place
@C { meet } into a child node of @C { target_meet }'s node.  In
exceptional circumstances, this may not be possible, and then the
pre-existing assignment is removed by @C { KheLayerTreeMake }.
Suppose there is an instance event with two meets, both
assigned to other meets.  If those two other
meets are both derived from the same instance event, or if they
are both cycle meets, then all is well; but if not, one
of the original meets will be deassigned.  This is done
because @C { KheLayerTreeMake } tracks relations between events,
not meets, and cannot cope with the idea of one event
being assigned partly to one event and partly to another.  A
meet will also be deassigned when there is a cycle of
assignments, but that should never occur in practice.
@PP
The above attempts to be a complete specification of
@C { KheLayerTreeMake }, sufficient for using it.  For the record,
the following subsections explain how it works in detail.
@BeginSubSections

@SubSection
    @Title { Overview }
    @Tag { layer_tree_solvers.construction.overview }
@Begin
@LP
# It is possible to assign random times to events and measure the
# cost, but more structure seems to be necessary for efficiency in
# practice.  @C { KheLayerTreeMake } rests on the thesis that the
# layer tree is the appropriate structure.  If this is correct, then
# the events of a typical instance may be structured as a layer tree
# in such a way that all or most of the relevant constraints are
# satisfied.  This is what @C {  KheLayerTreeMake} tries to do.
# @PP
@C { KheLayerTreeMake } uses a constructive heuristic which runs
quickly.  It works by examining the relevant constraints and
taking actions to satisfy them, giving priority to those with
higher weight.  It does not search through a large space of
possible solutions to find the best.  This is appropriate,
because in practice good solutions are easy to find.  The problem
is more about giving due weight to the many influences on the
solution than about real solving.
@PP
Before entering its main part, @C { KheLayerTreeMake } deassigns
meets to remove cases where two meets derived from a single event
are assigned to meets not both derived from the same event or
both cycle meets, and splits meets whose duration exceeds the
number of times in the instance into meets of duration within
that bound.  This allows the remainder of the algorithm to
assume that each event is preassigned to at most one other
event, and that there are no oversize meets.
@PP
After the main part, @C { KheLayerTreeMake } ensures that each
unassigned meet lies in a node, creating one if necessary, and
makes the promised calls to @C { KheMonitorAttachCheck }.
# @PP
# After completing its main part, @C { KheLayerTreeMake } carries out
# some simple operations to fulfil its promises about layers.  It
# ensures, for each event, that its meets cannot overlap
# in time:  if they are not already assigned to distinct solution
# events which cannot overlap in time, it adds them all to a new
# layer.  It removes redundant layers, and it ensures that each
# unassigned meet lies in at least one layer, by adding
# it to a new layer if necessary.
# @PP
# Second, it ensures that every event's meets have at least
# one layer in common, and that every unassigned meet lies
# in at least one layer.  This is done in the obvious way by creating
# layers and adding meets to them as required.  There is one
# wrinkle, however.  Function @C { KheLayerAddMeet } refuses
# to add a meet to a layer when that would cause two solution
# events from the layer to overlap in time, or cause the total duration
# of the layer to exceed the number of times in the instance.  To handle
# these unlikely cases, whenever this chapter says that a layer is
# created, in reality a set of layers is created.  Each meet
# is added to the first of these layers that will accept it; if none do,
# a new layer is begun, which must accept it, given that its duration
# does not exceed the number of times in the cycle.  Initially this set
# of layers is empty, and normally it grows to contain just one layer.
@PP
The remainder of this section describes the main part of the algorithm.
@PP
In practice, it is likely that the constraints of an instance will
cooperate harmoniously, but for completeness it is necessary to
handle cases where they do not.  For example, there is nothing to
prevent a link events constraint from linking two events, one of
which is required by a split events constraint to split into three
meets, while the other is required to split into one.
@PP
There is a data structure, described in the following sections,
which embodies all the requirements that the final layer tree
must satisfy, including how events are to be split into
meets, and how meets are to be grouped into nodes.
# and layers.
It is an invariant that at least
one layer tree must satisfy all these requirements.  Initially, the
data structure embodies no requirements at all.  A long series of
@I { jobs } is then applied to it, each inspired by some constraint
or other feature of the instance to request that the data structure
add some new requirements to the ones it currently embodies.  If no
layer trees would satisfy both the old and new requirements,
the job is @I { rejected } (it is ignored); otherwise, it is
@I { accepted } (its requirements are added).  There are also cases
in which some of the requirements of a job are accepted but others
have to be rejected.  The jobs are sorted by decreasing priority,
which is usually the combined weight of the constraint that inspired
the job.  In this way, contradictory requests are resolved by giving
preference to requests of higher priority.
@PP
Here is the full list of job types, with brief descriptions.  How
each job modifies the data structure will be explained later.  The
jobs not derived from constraints have high priority.
# @PP
# @I { Consistency of meets with the cycle layer. }
# It would not do to produce a meet of greater duration than
# any of the time blocks in the cycle layer.  Also, the number of solution
# events derived from the same event whose duration equals the cycle layer
# maximum should not exceed the number of blocks with that duration in the
# cycle layer.  These are special cases of a more general requirement
# imposed by this job:  the meets of each event must be
# packable into the time blocks of the cycle layer.
@PP
@I { Pre-existing splits. }  Each event @M { e } that is already split
generates one job, requiring the set of meets that @M { e }
is ultimately split into to be packable into (that is, created by
further splitting of) the pre-existing meets.
@PP
@I { Preassigned times. }  According to the XML specification, a
meet derived from an instance event with a preassigned
time must be assigned that time.  It is not likely that several
simultaneous meets derived from one instance event would
be wanted, so this job requests that a preassigned instance event
be not split further than its pre-existing splits, and that the
meets' time domains be set to domains returned by
@C { KheSolnPreassignedTimeDomain }.
@PP
@I { Pre-existing assignments and link events constraints. }  These are
interpreted as requests to create parent-child links between nodes.
@PP
# @I { Pre-existing layers and avoid clashes constraints. }  Each
@I { Avoid clashes constraints. }  Each resource subject to a
required avoid clashes constraint gives rise to a job which
requests that the layer tree recognize that the events to which
the resource is preassigned cannot overlap in time.
@PP
@I { Split events constraints and distribute split events constraints. }
These request restrictions on the number of meets that an
event may be split into, and their durations.
@PP
@I { Spread events constraints. }  If the events of an event group of
a spread events constraint are split into too many or too few meets,
then a non-zero number of deviations of the constraint becomes
inevitable.  The job tries to tighten the requirements on the number
of meets of the events concerned, to the point where this
problem cannot arise.
@PP
@I { Prefer times constraints. }  This kind of job requests that the
time domain of the meets of an event which have a certain
duration be reduced to satisfy a prefer times constraint.  This may
lead to an empty domain for meets of that duration; if so,
then there can be no meets of that duration at all, which
may prevent the job from being accepted.
@End @SubSection

@SubSection
    @Title { Linking }
    @Tag { layer_tree_solvers.construction.linking }
@Begin
@LP
The data structure used by @C { KheLayerTreeMake } must be close
enough to the layer tree to make it straightforward to derive an
actual layer tree at the end.  In fact, it needs to represent the set
of layer trees that satisfy the requirements of all the jobs accepted
so far.  This section explains how this is done for linking, and later
sections explain the parts that handle splitting and layering.
@PP
If meet @M { s sub 1 } can be assigned to meet
@M { s sub 2 } at offset @M { o sub 1 }, and @M { s sub 2 } can be
assigned to @M { s sub 3 } at offset @M { o sub 2 }, then it is
always possible to assign @M { s sub 1 } directly to @M { s sub 3 }
at offset @M { o sub 1 + o sub 2 }.  Thus, the relation of assignability
between meets is transitive.  Although it is not safe to
assign a meet to itself, it does no harm to pretend here
that assignability is reflexive as well.
@PP
In some cases, two meets are assignable to each other.  They
must have equal durations and time domains, but that is not unusual.
By a well-known fact about reflexive and transitive relations, two-way
assignability is an equivalence relation between meets.
@PP
Similar relations can be defined between events.  Let
@M { A( e sub 1 , e sub 2 ) } hold when the meets of
@M { e sub 1 } can be assigned to the meets of
@M { e sub 2 } at non-overlapping offsets.  Define
@ID @Math { S( e sub 1 , e sub 2 ) =
A( e sub 1 , e sub 2 ) wedge A( e sub 2 , e sub 1 ) }
Again, @M { A } is reflexive and transitive, and @M { S } is an
equivalence relation.
@PP
The data structure used for linking events includes a representation
of relations @M { A } and @M { S }.  The equivalence classes defined
by @M { S } are represented by nodes of a graph, containing the
events of the class and connected to other equivalence classes by
directed edges representing @M { A }.  @M { A } could be an arbitrary
directed acyclic graph, but in fact it is limited to a tree:  each
equivalence class is recorded as assignable to at most one other
equivalence class.  Relational nodes will always be called classes,
to avoid confusion with layer tree nodes.
@PP
The child classes of each equivalence class are organized into layers.  That
additional structure is not needed for linking, however, so its description
will be deferred to Section {@NumberOf layer_tree_solvers.construction.layer}.
@PP
Initially, each event lies in its own class, plus there is one class
with no events, representing the cycle meets.  Every event
class is a child of the cycle meets class.  Thus, initially
relation @M { S } is empty, and relation @M { A } records only the
basic fact that every event is assignable to the cycle meets
to begin with.  This is quite true, since, at this initial stage, before
any jobs are accepted, the data structure believes that each event's
domain is the entire cycle, that each event is free to split into
meets of duration 1, and that there are no layers.
@PP
Basing the data structure on events, rather than on meets, seems to
be right, but it does cause differences between the meets of one
event to be overlooked.  For example, the data structure believes
that all meets derived from the same event have the same time domain.
@PP
Jobs that link events together do so by proposing elements of
@M { A } and @M { S } to the data structure, which accepts them when
it can.  An @M { S } proposal is a request to merge the equivalence
classes containing its two events into one (if they are not already
the same); an @M { A } proposal is a request to replace one parent
link by another (which must still imply the first by transitivity).
A proposal could be rejected for various reasons:  it might lead to
a directed acyclic graph which is not a tree, or cause events from
the same layer to overlap in time, or lead to unacceptable
restrictions on how events are to be split (as in the example at
the start of this chapter), and so on.
@PP
Pre-existing assignments are proposed first as elements of @M { S },
and if that fails as elements of @M { A }.  The second proposal at
least cannot fail to be accepted, because these jobs have maximum
priority and do not contradict each other.  A link events constraint
job first proposes all pairs of linked events of equal duration as
elements of @M { S }, and then all pairs regardless of duration as
elements of @M { A }.  In general, an @M { A } proposal could require
that the whole set of classes lying on a cycle of @M { A } links be
evaluated for merging, but this particular way of making proposals
ensures that, in fact, only pairwise merges need to be evaluated.
@PP
Each equivalence class has a @I { class leader }, one of its
own events.  When an equivalence class is created, its leader
is the sole event it initially contains, and when two classes
are merged, one of the two leaders is chosen to be the leader of
the merged class.  For convenience, we pretend that the cycle
meets are derived from a single @I { cycle event }
which is the leader of their class.
@PP
If class @M { C } contains an event @M { e } which is assigned to
an event outside @M { C }, then the event @M { e } is assigned to
lies in the parent class of @M { C }.  There may not be two such
events in @M { C } unless they are assigned to the same event at
the same offset.  The leader must be one of these events.  The data
structure only becomes aware of assignments when the jobs
representing them are accepted.
@PP
If @M { C } does not contain an event which is assigned to another
event outside the class, then it must contain at least one event
which is not assigned at all, since otherwise there would be a
cycle of assignments within the class.  Any such unassigned
event may be the leader.
@PP
These conditions are trivially satisfied when a class is created,
by making its sole event the leader.  When two classes are merged,
there are various possibilities, including failure to merge when
the two leaders are assigned to distinct events outside both classes.
@PP
When constructing the final layer tree, all the unassigned events of each
class except the leader are placed in layer tree nodes which are made
children of the node containing the leader.  Similarly, the nodes
containing the leaders of child classes become children of the node
containing the leader of the parent class.  In reality, of course,
it is the meets derived from these events by the splitting
algorithm to be described next that are placed into these nodes.
@End @SubSection

@SubSection
    @Title { Splitting }
    @Tag { layer_tree_solvers.construction.split }
@Begin
@LP
Given an event @M { e } of duration @M { d }, any mathematical
partition of @M { d } is a possible outcome of splitting @M { e }.
For example, if @M { e } has duration 6, the possible outcomes
are the eleven partitions
@CD @OneCol lines @Break {
6
5 1
4 2
|2c
4 1 1
3 3
3 2 1
|2c
3 1 1 1
2 2 2
2 2 1 1
|2c
2 1 1 1 1 
1 1 1 1 1 1
}
# listed here in a natural lexicographical order.
One element of a partition is called a @I { part }, and is the
duration of one meet.
# @PP
# There is a natural lexical ordering on partitions.  Condition 
# @M { p sub 1 <= p sub 2 } holds if @M { p sub 1 } is empty, or
# both are non-empty and the largest part of @M { p sub 1 } is smaller
# than the largest part of @M { p sub 2 }, or their largest parts are
# equal and when both parts are removed, the condition applies to
# the remainders.  For example, @M { 3 3 <= 4 1 1 }.  Given a non-empty
# set @M { S } of partitions, a @I { lexically minimum } partition
# @M { p sub m } satisfies @M { p sub m <= p } for all @M { p in S }.
# It is easy to see that the relation is antisymmetric as well as
# reflexive and symmetric, so it is a total order and thus a lexically
# minimum element always exists.
@PP
Anything that influences how one event is split defines a subset of
this set of partitions.  For example, if a split events constraint
states that an event of duration 6 should be split into exactly four
meets, that is equivalent to requiring the partition to be
either @OneCol { 3 1 1 1 } or @OneCol { 2 2 1 1 }.
# In this way, the many influences on splitting are reduced to a
# common currency:  each is a job with a priority and a set of
# partitions for each event it influences.
@PP
Each equivalence class holds a set of events of equal duration
that are assignable to each other.  These will eventually be
partitioned into meets in the same way.  In addition to
the events, the class holds the requirements that the final
partition must satisfy.  These define a subset of the set of all
partitions of the duration, but it is not possible to store
the subset directly, because for large durations it may be very
large.  One partition @I is stored, however:  the lexically minimum
one satisfying the requirements.  (A lexically minimum partition
has minimum largest part, and so on recursively.  For example,
@OneCol { 1 1 1 1 1 1 } is the lexically minimum partition of 6.)
It is an invariant that the set of partitions satisfying the
requirements may not be empty.
@PP
In the special case of the equivalence class that represents the
cycle meets, the requirements are fixed to allow exactly one
partition:  the one representing the durations of the cycle meets.
@PP
The requirements on partitions are of two kinds.  First, there are
the @I { local requirements }.  These are mainly lower and upper
bounds on the total number of parts, and on the number of parts of
each possible duration, modelled on the corresponding fields of the
split events and distribute split events constraints.  Another kind
of local requirement arises when a pre-existing split job is
accepted:  if an event of duration 6 is already split into
meets of duration 4 and 2, say, when the algorithm begins, then, to
be acceptable, a partition must be packable into partition 4 2.  One
partition is @I packable into another if splitting some parts of the
second partition and discarding others can produce the first.  For
example, @OneCol { 2 1 1 } is packable into @OneCol { 2 2 2 }, but
neither of @OneCol { 3 1 1 1 } and @OneCol { 2 2 1 1 } is packable
into the other.
@PP
Second, there are the @I { structural requirements }.  Each parent
class has an arbitrary number of child classes, whose events will
eventually be assigned to the parent class's events.  So the lexically
minimum partition of each child class must be packable into the
parent class.  In these calculations the constraint always flows
upwards:  the child's lexically minimum partition is taken as
given, and the parent's minimum partition is adjusted (if possible)
to ensure that the child's is packable into it.  When a child
class's minimum partition changes, the parent's requirements must
be re-tested.  In this way, a change to a partition propagates
upwards through the structure until it either dies out or causes
some class to have no legal partitions.  In the second case, the
job which originated the changes must be rejected.
@PP
Some of the child classes may be organized into layers.  In that
case, each layer's classes, taken together, must be packable into
the parent class.  Each layer is represented by a split layer
object, as explained in detail in the next section.  That object
contains a minimum partition which must be packable into the parent
class, just like the minimum partitions of child classes.
@PP
Deciding whether any partitions satisfy even the local requirements
is non-trivial:  is it safe to place two events into one class, when
one is already split into partition @OneCol { 4 2 } and the other
is already split into partition @OneCol { 3 2 1 }?  Some simple
checks are made, then a full generate-and-test enumeration is begun
and interrupted at the first success.  The enumeration produces the
lexically minimum acceptable partition first, which is then stored
and propagated upwards.  Fortunately, packability can be tested very
quickly in practice, despite being an NP-complete bin packing problem,
because event durations are usually small.
@PP
At the end, after the last job is processed, each event of each
class is split into meets whose durations form the
minimum partition of that class.
#@PP
#@I { miscellaneous stuff, needing redistribution }
#@PP
#When assigning meets layer by layer, some of the
#meets of the current layer may already be assigned
#when that layer is reached, perhaps because the meet
#is preassigned, or because it lies in more than one layer, and
#one of its layers has already been assigned.  An algorithm which
#assigns times to the meets of a layer must take into
#account, and not disturb, any pre-existing assignments.
#@PP
#The @C { prefer_longer_durations }
#parameter of @C { KheSolnSplitLinkAndLayer } influences the choice of
#partition.  A heuristic attempt is made to coordinate the partitions
#chosen for the strong equivalence classes of each weak equivalence
#class, so that they have many parts in common, and also to coordinate
#the partitions across all strong equivalence classes of equal duration,
#for regularity.  Choosing a specific partition amounts to reducing the
#set of acceptable partitions to a singleton set, and the condition of
#packability of each layer into the cycle layer, described above,
#continues to be checked and preserved.
@End @SubSection

@SubSection
    @Title { Layering }
    @Tag { layer_tree_solvers.construction.layer }
@Begin
@LP
The relation between meets and layers (sets of events that share
a common preassigned resource with a required avoid clashes
constraint) is a many-to-many relation:  a layer may contain any
number of meets, and a meet may lie in any
number of layers.
# @PP
# As mentioned above, the data structure is based on instance events,
# not meets, and so it keeps track of a many-to-many
# relation between instance events and layers.  It has no idea that
# some of the meets of an instance event might lie in some
# layer, and others not.  If at least one meet derived from
# some instance event lies in some layer, then the data structure
# believes that they all do.  In the usual initial state, there is
# only one meet per event, and no layers except the cycle
# layer, so this is unlikely to cause problems in practice.
@PP
Suppose that meet @M { s sub 1 } lies in layer @M { l }
and is assigned to meet @M { s sub 2 }.  KHE enforces
the rule that any assignment of @M { s sub 2 } may not be such
as to cause @M { s sub 1 } to overlap in time with any other
meet of @M { l }.  In a sense, @M { s sub 2 } (actually,
that part of it assigned @M { s sub 1 }) becomes a member of
@M { l } while @M { s sub 1 } is assigned to it.  We say that
@M { s sub 1 } lies @I directly in @M { l }, and @M { s sub 2 }
lies @I indirectly in @M { l }.
@PP
An event lies directly in a layer if any of its meets
lie directly in the layer.  An equivalence class lies directly in
a layer if any of its events lie directly in the layer, and it lies
indirectly in the layer if any of its child classes lie in the layer,
either directly or indirectly.  This is because the events of child
classes will eventually be assigned to the events of the class.
@PP
The layering aspect of @C { KheLayerTreeMake } is based on an object
called a @I { split layer }, which represents one element of the
many-to-many relation between equivalence classes and layers.  In other
words, there is one split layer object for each case of an equivalence
class lying in a layer, directly or indirectly.  Its attributes are the
class, the resource defining the layer, the set of all child classes
of the class that lie in the layer, and a partition, whose value will
be defined shortly.
@PP
When an equivalence class lies directly in a layer (when it contains
an event that lies directly in the layer), none of its child classes
can lie in the layer, since that would mean that two events of the
same layer overlap in time.  So in that case the set of child classes
must be empty.  To keep it that way, the partition contains as many
1's as the duration of the class.  This makes it clear that there is
no room for any child classes in the layer, without constraining the
division of the class's events into sub-events in any way.
@PP
When an equivalence class lies indirectly in a layer, some of its
child classes lie in the layer.  Their total duration must not
exceed the duration of the class, and their meets, taken
together, must be packable into the class, since they are disjoint
in time.  So in this case the set of child classes may be (in fact,
must be) non-empty, and the partition holds the multiset union of
the lexically minimum partitions of the child classes.
@PP
The job which adds a layer to the data structure adds its events
one by one.  In the unlikely event that the duration of the layer
exceeds the number of times in the cycle, or bin packing problems
prevent an event being added, the job rejects the event, which
amounts to ignoring the presence of the preassigned resource in
that event.
# @PP
# In order to handle awkward cases gracefully, the job which adds a
# layer to the data structure actually adds a set of layers.  Usually
# the set turns out to contain just one layer, as desired, but it may
# contain more.  Initially the job's set of layers is empty.  For each
# event, the job requests that it be added to each layer of its set in
# turn, until either a request succeeds or the layers have all been tried.
# In the second case, a new layer is created and added to the set, and the
# request is made again with that layer; this must succeed.  (This approach
# is modelled on a well-known bin packing heuristic, although the events
# are not sorted beforehand.)
@PP
Adding an event to a layer means that the event's class and all
its ancestors must get split layer objects for the layer.  For
all these classes, moving upwards until either there are no more
ancestors or a class already has a split layer object for the layer,
either add a new split layer object holding just the current child
class, or add the child class to an existing split layer object.
@PP
While the upward propagation adds new split layer objects, there is
no possibility of failure, since a layer containing a single event is
no more constraining than the event alone (the event is already
present, only its membership of a layer is changing).  But if an
existing split layer object is reached, the class must be added to
it, and so its partition grows, possibly leading to an empty set of
acceptable partitions in the parent, causing rejection of the request.
# @PP
# Second, it ensures that every event's meets have at least
# one layer in common, and that every unassigned meet lies
# in at least one layer.  This is done in the obvious way by creating
# layers and adding meets to them as required.  There is one
# wrinkle, however.  Function @C { KheLayerAddMeet } refuses
# to add a meet to a layer when that would cause two solution
# events from the layer to overlap in time, or cause the total duration
# of the layer to exceed the number of times in the instance.  To handle
# these unlikely cases, whenever this chapter says that a layer is
# created, in reality a set of layers is created.  Each meet
# is added to the first of these layers that will accept it; if none do,
# a new layer is begun, which must accept it, given that its duration
# does not exceed the number of times in the cycle.  Initially this set
# of layers is empty, and normally it grows to contain just one layer.
@End @SubSection

@EndSubSections
@End @Section

@Section
    @Title { Vizier nodes }
    @Tag { layer_tree_solvers.vizier }
@Begin
@LP
A @I vizier (Arabic @I { wazir }) is a senior official, often the
one who actually runs the country while the nominal ruler gets
the adulation.  In a similar way, a @I { vizier node } sits below
another node and does what that other node nominally does:  act
as the common parent of the subordinate nodes, and hold the meets
that those nodes' meets assign themselves to.  A vizier node is
not different from any other node; only its role is special.
@PP
Any node can have a vizier, but only the cycle node really has
a use for one.  By connecting everything to the cycle node
indirectly via a vizier, it becomes trivial to try time repairs
in which the meets of the vizier node change their assignments,
effecting global alterations such as swapping everything on
Tuesday morning with everything on Wednesday morning.  Function
@ID @C {
KHE_NODE KheNodeInsertVizierNode(KHE_NODE parent_node);
}
inserts a new vizier node directly below @C { parent_node },
and returns the new node.  For every meet @C { pm } of the
parent node, the vizier has one meet @C { vm } with the same
duration as @C { pm } and assigned to @C { pm } at offset 0.
The domain of @C { vm } is @C { NULL }.  Each child node of
@C { parent_node } becomes a child of the vizier, and each
meet assigned to a meet of the parent node becomes assigned
to the corresponding meet of the vizier.  If @C { parent_node }
has zones, the vizier is given corresponding zones, and the
parent node's zones are removed.
@PP
All this leaves the timetable unchanged, including constraints
imposed by domains and zones.  The vizier takes over without
affecting anyone's existing rights and prvileges.
@PP
To undo the effect of an earlier call to 
@C { KheNodeInsertVizierNode }, call
@ID @C {
void KheNodeRemoveVizierNode(KHE_NODE vizier_node);
}
What this mainly does is call @C { KheNodeBypass(vizier_node) }
from Section {@NumberOf layer_tree_solvers.flattening}, followed
by @C { KheNodeDelete(vizier_node) }; but first it does its
best to create zones in the parent node corresponding to the
zones in @C { vizier_node }, based on the assignments of the
vizier's meets.  It also deletes @C { vizier_node }'s meets.
@PP
Functions @C { KheNodeMeetSplit } and @C { KheNodeMeetMerge }
(Section {@NumberOf extras.nodes.meet_split}) are particularly
relevant to vizier nodes.  Splitting a vizier's meets
non-recursively opens the way to fine-grained swaps, between
half-mornings instead of full mornings, and so on.  And
@C { KheNodeMeetMerge } is the appropriate way to return
the vizier meets to their original number and durations.
@End @Section

@Section
    @Title { Layers }
    @Tag { layer_tree_solvers.layers }
@Begin
@LP
Layers were introduced in Section {@NumberOf extras.layers}, but
no easy way to build a set of layers was provided.  This section
remedies that deficiency and adds some useful aids to solving
with layers.
@BeginSubSections

@SubSection
    @Title { Layer construction }
    @Tag { layer_tree_solvers.layerings }
@Begin
@LP
The usual rationale for the existence of a layer is that its
nodes' meets must not overlap in time because they
contain preassignments of a common resource.  Function
@ID @C {
KHE_LAYER KheLayerMakeFromResource(KHE_NODE parent_node,
  KHE_RESOURCE r);
}
builds a layer of this kind.  It calls @C { KheLayerMake } to
make a new child layer of @C { parent_node }, and
@C { KheLayerAddResource } to add @C { r } to this layer.  Then,
each child node of @C { parent_node } which contains a meet
preassigned @C { r } (either directly within the node, indirectly
within descendant nodes, or in meets assigned, directly or
indirectly, to those meets) is added to the layer.
@PP
The @I { layering } of node @C { parent_node } is a particular set
of layers which is useful when assigning times to the child nodes
of @C { parent_node }, created by calling function
@ID @C {
void KheNodeChildLayersMake(KHE_NODE parent_node);
}
This will delete any existing child layers of @C { parent_node }
and add the layers of the layering.  
@PP
The layering is built as follows.  First, for each resource of
the instance that possesses a required avoid clashes constraint,
one layer is built by calling @C { KheLayerMakeFromResource }
above.  If it turns out to be empty, it is immediately deleted
again.  Each pair of these layers such that one's node set is
a subset of the other's is merged with @C { KheLayerMerge }.
Finally, each child of @C { parent_node } not in any layer goes
into a layer (with no resources) by itself, and the layers are sorted
by decreasing duration, with ties broken in favour of increasing
number of nodes.
@PP
The sorting is done by calling @C { KheChildLayersSort } with
this comparison function:
@ID { 0.94 1.0 } @Scale @C {
int KheLayerDefaultCmp(const void *t1, const void *t2)
{
  KHE_LAYER layer1 = * (KHE_LAYER *) t1;
  KHE_LAYER layer2 = * (KHE_LAYER *) t2;
  if( KheLayerDuration(layer1) != KheLayerDuration(layer2) )
    return KheLayerDuration(layer2) - KheLayerDuration(layer1);
  else if( KheLayerChildNodeCount(layer1) != KheLayerChildNodeCount(layer2) )
    return KheLayerChildNodeCount(layer1) - KheLayerChildNodeCount(layer2);
  else
    return KheLayerIndex(layer1) - KheLayerIndex(layer2);
}
}
As a last resort, the index number of the layer is used to break ties.
# Other examples of layer comparison functions may be found in
# Section {@NumberOf time_solvers.soln_layer.first}.
# @PP
# As an aid to debugging, KHE offers function
# @ID @C {
# void KheNodeChildLayersDebug(KHE_NODE parent_node, int verbosity,
#   int indent, FILE *fp);
# }
# It sends a debug print of the layers to @C { fp } in the usual way.
@PP
Two important facts about layers and layerings must be borne in
mind.  First, they reflect the state of the layer tree at a
particular moment.  If, after they are built, the tree is
restructured (if nodes are moved, etc.) they become out of date
and useless.  Second, building a layering is slow and should not
be done within the inner loops of a solver.
@PP
Altogether, it seems best to regard layers as temporary structures,
created when required by @C { KheChildLayersMake } and destroyed
by @C { KheChildLayersDelete }.  In between these two calls, nodes
may be merged and split, but it is best not to move them.  A useful
convention, supported by several of KHE's solvers that use layers,
is to assume that if child layers are present, then they are up to
date.  Such solvers begin by calling @C { KheChildLayersMake } if
there are no layers, and end by calling @C { KheChildLayersDelete },
but only if they called @C { KheChildLayersMake }.
@End @SubSection

@SubSection
    @Title { Layer coordination }
    @Tag { layer_tree_solvers.layer.coordination }
@Begin
@LP
# This section presents a solver for coordinating similar layers.  The
# solver itself is quite simple, but its purpose needs some explanation.
# @PP
High schools usually contain @I { forms } or @I { years }, which are
sets of students of the same age who follow the same curriculum, at
least approximately.  These students may be grouped into classes,
each represented by one student group resource.  At some times, the
student group resources of one form might attend the same events,
or linked events.  For example, they might all attend a common
Sport event, or they might all attend Mathematics at the same
times so that they can be regrouped by ability at Mathematics.
At other times, they might attend quite different events, but over
the course of the cycle they all attend the same amount of each
different kind of event:  so many times of English, so many of
Science, so many of a shared elective, and so on.
@PP
As an aid to producing a regular timetable, it might be helpful to
@I coordinate the timetables of student groups from the same form:
run all the form's English classes simultaneously, all its
Mathematics classes simultaneously, and so on.  Where resources
are insufficient to support this, changes can be made later.  In
this way, a regular timetable is produced to begin with, and
irregularities are introduced only where necessary.
@PP
The XML format does not explicitly identify forms, or even say
which resource type contains the student group resources.  This is
in fact an advantage, because it forces us to look for structure
that aids regularity.  We then coordinate the timetabling of
resources that possess the useful structure, without knowing or
caring whether they are in fact student group resources.
@PP
Coordination will only work when the chosen resources attend
similar events.  This was the rule when inferring resource
partitions (Section {@NumberOf resources_infer}), so we take the
resource partition as the structural equivalent of the form.  The
events should occupy all or most of the times of the cycle,
otherwise coordination eliminates too many options for spreading
them in time.  `Forms' of teachers and rooms are rarely useful,
just because they do not satisfy these conditions.
@PP
After @C { KheLayerTreeMake } returns, it is the nodes lying
directly below the root node that need to be coordinated, not
events or meets.  Two child nodes may be coordinated
by moving one of them so that it is a child node of the other.
KHE offers solver function
@ID { 0.98 1.0 } @Scale @C {
void KheCoordinateLayers(KHE_NODE parent_node, bool with_domination);
}
which carries out such moves on some of the children of
@C { parent_node }, as follows.
@PP
@C { KheCoordinateLayers } is only interested in resources whose
layers have duration at least 90% of the duration of
@C { parent_node }.  For each pair of such resources lying
in the same resource partition, it checks whether their two
layers are similar by building the layers with
@C { KheLayerMakeFromResource } and calling @C { KheLayerSimilar }
(Section {@NumberOf layer_tree_solvers.layers}).  If so, it uses
@C { KheNodeMove } (Section {@NumberOf extras.nodes.move}) to
make each node of the second layer a child of the corresponding
node of the first layer, except when the two nodes are the same
node, obviously.  This forces these nodes to be simultaneous.  It
does not assign any meets, or remove them from their nodes.
Finally, it removes the two layers it made.
# @ID @C {
# void KheCoordinateSegments(KHE_NODE node, bool with_domination);
# }
# which coordinates the timetables of the segments of @C { node }.  It
# examines those segments of @C { node } whose layer has a resource that
# lies in a resource partition, and whose duration is at least 90% of
# the duration of @C { node }.  For each pair of such segments whose
# resources lie in the same partition, it checks whether the two
# segments are similar, by calling @C { KheSegmentSimilar } from
# Section {@NumberOf layer_trees.nodes.segments}.  If so, it coordinates
# their timetables by using @C { KheNodeMove } from
# Section {@NumberOf layer_trees.nodes.move} to make each child node
# of the second segment into a child of the corresponding child node
# of the first segment, except when the two nodes are the same node,
# obviously.  This forces these nodes to be simultaneous.  It does
# not assign any meets, or remove them from their nodes.
@PP
If @C { with_domination } is @C { false }, the behaviour is as
described.  If @C { with_domination } is @C { true }, a slight
generalization is used.  Suppose that one of the two
layers has duration equal to the duration of @C { parent_node },
and all but one of its nodes is similar to some node in the other
layer.  Then the dissimilar nodes of the other
layer (possibly none) might as well be made children of the one
dissimilar node of that layer, since if the other nodes
are coordinated they must run simultaneously with it anyway.  (The
durations of their meets may be incompatible; that is
not checked at present, although it should be.)  So that is done.
@PP
In unusual cases the duration of a layer can be larger after
coordinating than before.  At the end, if any layers have duration
larger than the parent node's duration, @C { KheCoordinateLayers }
tries to reduce the duration of those layers to the parent node's
duration, by finding cases where one node of a layer can be safely
moved to below another.
# @PP
# @C { KheCoordinateSegments } does not change the set of segments of
# @C { node }, but their order may change, and their child nodes change.
@End @SubSection

@EndSubSections
@End @Section

@Section
    @Title { Runarounds }
    @Tag { layer_tree_solvers.runarounds }
@Begin
@LP
Layer coordination can lead to problems assigning resources.
For example, suppose that the five student groups of the Year 7
form each attend one Music event, and that the school has two Music
teachers and two Music rooms.  Each event is easily accommodated
individually, but when the Year 7 layers are coordinated, they
run simultaneously and exceed resource limits.
@PP
These problems do not arise in large faculties with sufficient
resources to accommodate an entire form at once.  Thus they do not
invalidate the basic idea of node layer coordination.  What is needed
is a local fix for these problems.  This is what @I { runarounds }
provide:  a way to spread the events concerned through the times
they need, without abandoning coordination altogether.
@BeginSubSections

@SubSection
    @Title { Minimum runaround duration }
    @Tag { layer_tree_solvers.runarounds.minduration }
@Begin
@LP
Consider the case above where there are not enough Music resources
to run the Year 7 Music events simultaneously.  If these events lie
in nodes that are children of a common parent (one may lie in the
parent itself), it is easy to detect this problem:  carry out a
time assignment at the parent, and see whether the cost of the
solution increases.  This is assuming that the matching monitors,
which detect unsatisfiable resource demands, are attached.
@PP
More generally, we can ask how large the duration of the parent node
has to be in order to ensure that there is no cost increase.  This
quantity is called the @I { minimum runaround duration } of the node.
It will be equal to the duration when there is no problem, and larger
when there is a problem.  It can be calculated as follows.  While a
time assignment of the child nodes produces a state of higher cost
than the unassigned state, add new meets to the parent
node.  The duration of the parent node when this process ends is
its minimum runaround duration.  Function
@ID @C {
bool KheMinimumRunaroundDuration(KHE_NODE parent_node,
   KHE_TIME_SOLVER time_solver, int *duration);
}
sets @C { *duration } to the minimum runaround duration of
@C { parent_node } and returns @C { true }, except in an unlikely
case, documented below, when it returns @C { false } with
@C { *duration } undefined.
@PP
@C { KheMinimumRunaroundDuration } starts by deassigning all the child
meets and recording the unassigned cost.  It then carries
out the sequence of time assignment trials just described.  For each
trial after the first it adds one fresh meet to @C { parent_node }
for each of its original meets, utilizing their durations and
time domains, but with no event resources.  Thus the result must be a
multiple of the duration of @C { parent_node }.  Before returning, it
deassigns all the children and removes the meets it added,
leaving the tree in its initial state, unless some child
meets were assigned to begin with.
@PP
Parameter @C { time_solver } is a time assignment solver which
is called to carry out each trial.  A simple solver, such as
@C { KheSimpleAssignTimes } from Section {@NumberOf time_solvers.basic},
should be sufficient here.
@PP
Increasing the duration at each trial by the full duration of the
node may seem excessive, and there are cases where fewer additional
meets would be enough.  However, those cases require the child nodes'
assignments to overlap in ways that do not work out well in practice,
because they may lead to split assignments in the tasks affected.
@PP
How many trials are needed?  In reasonable instances, each child node's
duration should be no greater than the parent node's duration.  Thus,
after as many trials as there are child nodes plus one, there should
be enough room in the parent node to assign every child meet at an offset
which does not overlap with any other, or with the original parent meets.
This is the number of trials that @C { KheMinimumRunaroundDuration }
carries out.  It stops early if one succeeds with cost no greater than
the unassigned cost.  It returns @C { false } only when each trial
either did not assign all the child meets (that is, the call on
@C { time_solver } returned @C { false }) or did assign them all,
but at a higher cost than the unassigned cost.
@End @SubSection

@SubSection
    @Title { Building runarounds }
    @Tag { layer_tree_solvers.runarounds.construct }
@Begin
@LP
Nodes may be classified into three types.  A @I { fixed node } has
no child nodes.  There is no possibility of spreading the events
of a fixed node and its descendants through more times than the
node's duration.  A @I { problem node } has minimum runaround
duration larger than its duration, like the node of Music events
used as an example above.  It must have child nodes, and
timetabling them simultaneously is known to be inferior to spreading 
them out further.  The remaining nodes are @I { free nodes }:  they
have child nodes which may run simultaneously, or not, as convenient.
@PP
Using @C { KheNodeMerge } to merge problem nodes with other problem
nodes and free nodes can eliminate problem nodes without greatly
disrupting regularity.  For example, merging a Music problem node
of duration 2 and minimum runaround duration 6 with a free node of
duration 4 produces a merged node of duration 6 which can usually
be timetabled without problems.
@PP
If a merged node can be timetabled without the cost of the solution
increasing, it may be kept, and is then called a @I { runaround node }.
(The term @I runaround is used by manual timetablers known to the
author to describe this kind of timetable, where events like the
Music events are `run around' with other events.)  Otherwise it must
be split up again and some other merging tried instead.  It only
remains, then, to decide which sets of nodes to try to merge.
@PP
Regularity is easier to attain when nodes have the same duration,
so if there are already many nodes of a certain duration, it is
helpful if a merged node also has that duration.  Nevertheless, a
node should not be added to a merge merely to make up some duration:
merging limits the choices open to later phases of the solve, so
it should be done only when necessary.
@PP
A minimum runaround duration could be very large, close to the
duration of the whole cycle.  For example, suppose there is a
single teacher, the school chaplain, who gives each of the five
Year 7 student groups 6 times of religious instruction per week.
Those events have a minimum runaround duration of 30.  When the
minimum runaround duration of a node is larger than a certain value,
the algorithm given below ignores the node:  its events will be
awkward to timetable, but runarounds as defined here are not the
answer.
@PP
To build runaround nodes from the child nodes of
@C { parent_node }, call
@ID @C {
void KheBuildRunarounds(KHE_NODE parent_node,
  KHE_TIME_SOLVER mrd_solver, KHE_TIME_SOLVER runaround_solver);
}
where @C { mrd_solver } is passed to @C { KheMinimumRunaroundDuration } when
minimum runaround durations need to be calculated, and @C { runaround_solver }
is called to timetable merged nodes.  @C { KheSimpleAssignTimes } is
sufficient for @C { mrd_solver }, and @C { KheRunaroundNodeAssignTimes }
works well as @C { runaround_solver }.  All nodes are unassigned afterwards.
@PP
It would not do to merge (for example) a node that includes both Year
7 and Year 8 events with a node that includes only Year 7 ones.  So
@C { KheBuildRunarounds } first works out which resources are preassigned
to events in or below which nodes (taking account only of preassigned
resources which have required avoid clashes constraints, and whose
events occupy at least 90% of the duration of @C { parent_node }), and
partitions the child nodes of @C { parent_node } into disjoint subsets,
such the nodes in each subset have the same preassigned resources.
@PP
For each disjoint subset independently, @C { KheBuildRunarounds }
tries to build a merged node around each of the subset's problem
nodes in turn, largest minimum runaround duration first.  When doing
this, it prefers to build a node of a particular duration @M { u },
and it prefers to use other problem nodes (again, largest minimum
runaround duration first), but it will also use free nodes
(minimum duration first).  It is heuristic, but it usually works
well.  It is not limited to sequences of pairwise mergings, as
clustering algorithms often are.  Here is the algorithm in detail:
@NumberedList

@LI {
The input is a set of nodes @M { N } (one disjoint subset as above),
plus @M { u }, a desirable duration for a merged node, and @M { v },
a maximum duration for a merged node.  The output is @M { M }, the
final set of nodes.  Write @M { d(n) } for the duration of node
@M { n }, @M { r(n) } for its minimum runaround duration, and
@M { d(X) } for the total duration of the set of nodes @M { X }.
}

@LI {
Initialize @M { M } to empty.  Sort @M { N } to put free nodes first,
in decreasing duration order, problem nodes next, in increasing minimum
runaround duration order, and fixed nodes last.
}

@LI @Tag { looptop } {
If @M { N } is empty, stop.  Otherwise delete the last element of
@M { N } and call it @M { n }.
}

@LI {
If @M { n } is fixed, problem with @M { r(n) >= v }, or free,
move it to @M { M } and return to Step {@NumberOf looptop}.
}

@LI {
Here @M { n } must be a problem node satisfying @M { r(n) < v }.
Within each of the following cases, some non-empty subsets @M { X }
of @M { N } are defined.  In each case, @M { r(n) <= d(n) + d(X) }, so
a merged node consisting of @M { n } merged with @M { X } is likely
to work well.  For each case in turn, and for each set @M { X }
defined within each case in turn, remove @M { X } from @M { N }, merge
@M { n } and @M { X }, and timetable the resulting merged node.
If that is successful (all events timetabled with no increase in
solution cost), add the merged node to @M { M } and return to
Step {@NumberOf looptop}.  If it fails, split the merged node up again,
return the nodes of @M { X } to their former places in @M { N }, and
try the next set @M { X }; or if there are no more sets, add @M { n }
to @M { M } and return to Step {@NumberOf looptop}.
@LeftList

@LI { Case 1.
For each @M { x in N } from last to first such that
@M { r(n) <= d(n) + d(x) = u <= v }, let @M { X = lbrace x rbrace }.
}

@LI { Case 2.
For each @M { i } from @M { 1 } to @M { "|" N "|" } such that
@M { X sub i }, the last @M { i } elements of @M { N }, satisfies
the condition @Math { r(n) <= d(n) + d( X sub i ) <= v },
let @M { X = X sub i }.
}

@RawEndList
}

@EndList
@C { KheBuildRunarounds } calls @C { KheMinimumRunaroundDuration } to
find minimum runaround durations, passing @C { mrd_solver } to it.  It
calls @C { KheNodeMerge } to merge nodes, @C { runaround_solver }
to timetable merged nodes, and @C { KheNodeSplit } to undo failed
merges.  It uses one-fifth of the duration of @C { parent_node } for
@M { v }.  For @M { u }, it builds a frequency table of the durations
of child nodes of @C { parent_node }.  It then chooses the duration
for which the frequency times the duration is maximum.  This weights
the choice away from small durations, which are not very useful.
@End @SubSection

@EndSubSections
@End @Section

@Section
    @Title { Flattening }
    @Tag { layer_tree_solvers.flattening }
@Begin
@LP
Although layer coordination and runaround building are useful
for promoting regularity, there may come a point where these
kinds of voluntary restrictions prevent assignments which satisfy
more important constraints, and so they must be removed.
@PP
What is needed is to flatten the layer tree.  Two functions are
provided for this.  The first is
@ID @C {
void KheNodeBypass(KHE_NODE node);
}
This requires @C { node } to have a parent, and it moves the children
of @C { node } so that they are children of that parent.  The second is
@ID @C {
void KheNodeFlatten(KHE_NODE parent_node);
}
It moves nodes as required to ensure that all the proper descendants
of @C { parent_node } initially are children of @C { parent_node }
on return.
@PP
Both functions use @C { KheNodeMove } to move nodes.  They cannot fail,
because @C { KheNodeMove } fails only when there is a problem with the
cycle rule, which cannot occur here.  Both functions are `interesting
exceptions' (Section {@NumberOf extras.nodes.move}) where assignments
are preserved.  By convention (Chapter {@NumberOf time_solvers}),
meets with fixed, final assignments should not lie in nodes.  If that
convention is followed, these functions do not affect such meets.
@End @Section

@Section
    @Title { Merging adjacent meets }
    @Tag { layer_tree_solvers.merging }
@Begin
@LP
It sometimes happens that at the end of a solve, two meets derived
from the same event are adjacent in time and not separated by a
break.  If the same resources are assigned to both, they can be
merged, which may remove a spread defect and thus reduce the overall
cost.  Function
@ID @C {
void KheMergeMeets(KHE_SOLN soln);
}
carries out all merges that reduce solution cost.  For each event
@C { e }, it takes the meets derived from @C { e } that have assigned
times and sorts them chronologically.  Then, for each pair of adjacent
meets in the sorted order, it tries @C { KheMeetMerge }, keeping the
merge if it succeeds and reduces cost.
@PP
@C { KheSolnMergeMeets } can be called at any time.  The best
time to call it is probably at the very end of solving, or
possibly after time assignment.
@End @Section

@EndSections
@End @Chapter
